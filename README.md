# M06 Spark Basics â€“ Python (GCP)

This project demonstrates an end-to-end Spark ETL pipeline deployed on Kubernetes, using GCP and Terraform infrastructure. It fulfills the requirements of the M06 Spark Basics Homework.
## ðŸ“Œ Project structure

M06_SparkBasics_Python_GCP/
â”œâ”€â”€ etl_job.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
        terraform/
                .terraform/terraform.tfstate
â”œâ”€â”€ charts/
â”‚   â””â”€â”€ spark-operator-chart/
â”‚       â”œâ”€â”€ values.yaml
â”‚       â””â”€â”€ templates/
â”‚           â””â”€â”€ deployment.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ *.png
â””â”€â”€ .gitignore


## ðŸ“Œ Steps Completed

### âœ… 1. Terraform Infrastructure (GCP)
- Provisioned GCS bucket: `m06-sparkbasics-gcp-tfstate`
- Provisioned GKE cluster
- Configured with `gcloud auth login`, `gcloud auth configure-docker`, `kubectl`

### âœ… 2. Spark ETL Job
- `etl_job.py` reads **hotels CSV** and **weather Parquet** data
- Uses OpenCage API to enrich hotels with missing coordinates
- Generates **4-character geohash**
- Joins with weather data on geohash (left join)
- Outputs **enriched Parquet dataset partitioned by year/month/day**

### âœ… 3. Docker
- Built custom image `spark-operator:local`
- Pushed to GCR: `gcr.io/m06-sparkbasics-gcp/spark-operator:local`

### âœ… 4. Spark Operator on Kubernetes
- Deployed using Helm chart from `spark-on-k8s-operator`
- Fixed CRDs, image reference, and configuration
- Cleaned up `.gitignore` and removed nested Git issues

### âœ… 5. Final Artifacts
- âœ… Enriched `.parquet` file in GCS
- âœ… Terraform infrastructure
- âœ… Spark ETL job (local + GKE)
- âœ… Working Docker image
- âœ… Clean GitHub repo

---

## ðŸ“¸ Screenshots to Upload
> Save these from PowerShell or browser, and add them to your repo `screenshots/` folder

1. âœ… Terraform Apply success in PowerShell
2. âœ… Spark ETL job output logs
3. âœ… `gcloud` config list
4. âœ… Parquet files uploaded in GCS bucket (`data/`)
5. âœ… `kubectl get pods` showing the Spark job running or completed
6. âœ… Docker image build + push
7. âœ… GitHub repo structure

---



